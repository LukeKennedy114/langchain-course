{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKNLIBKjZ7K1uowTfcqPFj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamurAIGPT/langchain-course/blob/main/chains/Chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains\n",
        "\n",
        "Chains are the core of LangChain. They are simply a chain of components, executed in a particular order.\n",
        "\n",
        "Langchain supports a bunch of chains each with it's own functionality. The easiest of these chains is the LLMChain . It works by taking a user's input, passing in to the first element in the chain — a PromptTemplate — to format the input into a particular prompt."
      ],
      "metadata": {
        "id": "9sM36nIUwvhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLMChain\n",
        "\n",
        "LLMChain is most basic chain in Langchain which we will talk about.\n",
        "\n",
        "It takes in a prompt template, formats it with the user input and returns the response from an LLM.\n",
        "\n",
        "#### What is the advantage of using this chain ?\n",
        "\n",
        "Using templates as input we can dynamically change what input we provide to LLM and thus use it directly without writing from scratch. Let's discuss LLMChain with an example"
      ],
      "metadata": {
        "id": "MukosxspiHgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai tiktoken chromadb"
      ],
      "metadata": {
        "id": "M5h56oEFjXyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PbdnRkUqvH6Q"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-key\"\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Give me a tweet idea on {topic}?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"AI\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEDIHoFsjUpg",
        "outputId": "9bb47afa-3d64-4dbe-bb91-d264e8a62d78"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "AI is changing the way we live! Get ready for the future with AI-powered technology. #AI #FutureTech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's extend the same concept for multiple variables as input"
      ],
      "metadata": {
        "id": "suU9b4CRkPmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic1\", \"topic2\"],\n",
        "    template=\"Give me a tweet idea on {topic1} and {topic2}?\",\n",
        ")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "print(chain.run({\n",
        "    'topic1': \"AI\",\n",
        "    'topic2': \"NLP\"\n",
        "    }))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iez9G73Hjl0e",
        "outputId": "f8047569-1978-46c9-e294-58322784619e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"AI and NLP are transforming the way we interact with machines. Learn more about the exciting possibilities and implications of this technology!\" #AI #NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential Chain\n",
        "\n",
        "A sequential chain works by combining two or more chains. For example you can take the output from one chain and pass it across to next chains sequentially\n",
        "\n",
        "Let's try to understand it with the help of an example"
      ],
      "metadata": {
        "id": "OcFXfTTgloIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we create a LLMChain to write outline for a blog topic"
      ],
      "metadata": {
        "id": "0kLqBGnDn6b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=.7)\n",
        "template = \"\"\"Write a blog outline given a topic.\n",
        "\n",
        "Topic: {topic}\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
        "outline_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"outline\")"
      ],
      "metadata": {
        "id": "xz2UbjHekLT5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we create another LLMChain which will take the output from above chain as input and creates a blog article based on the generated outline"
      ],
      "metadata": {
        "id": "6roHTswvn_J7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=.7)\n",
        "template = \"\"\"Write a blog article based on the below outline.\n",
        "\n",
        "Outline:\n",
        "{outline}\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"outline\"], template=template)\n",
        "article_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"article\")"
      ],
      "metadata": {
        "id": "zzzJOakWmmJO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally let's connect both the chains using a SequentialChain which has topic as input and outline, article as output"
      ],
      "metadata": {
        "id": "6VXf0eoMokgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[outline_chain, article_chain],\n",
        "    input_variables=[\"topic\"],\n",
        "    output_variables=[\"outline\", \"article\"],\n",
        "    verbose=True)"
      ],
      "metadata": {
        "id": "tlFKIKdanH0D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain({\"topic\":\"Deep Learning\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WJC0omAnVf7",
        "outputId": "a0459e36-0439-4abe-a5a2-07ddf396e66d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'Deep Learning',\n",
              " 'outline': '\\n\\nI. Introduction \\nA. Definition of deep learning \\nB. Overview of deep learning \\n\\nII. History of Deep Learning \\nA. Milestones in the development of deep learning \\nB. Pioneers in deep learning \\n\\nIII. Applications of Deep Learning \\nA. Uses in computer vision \\nB. Uses in natural language processing \\nC. Uses in robotics \\n\\nIV. Advantages of Deep Learning \\nA. Increased accuracy \\nB. Faster processing times \\nC. Lower costs \\n\\nV. Challenges of Deep Learning \\nA. Complexity of algorithms \\nB. Availability of data \\nC. Lack of interpretability \\n\\nVI. Conclusion \\nA. Summary of deep learning \\nB. Impact of deep learning on the future',\n",
              " 'article': ' \\n\\nDeep learning has revolutionized the way machines learn and interact with the world. It has a wide range of applications from computer vision to robotics. But what exactly is deep learning and how has it evolved over time? In this article, we’ll explore the definition, history, applications, advantages, and challenges of deep learning. \\n\\nAt its core, deep learning is a type of machine learning that uses artificial neural networks, or networks of connected algorithms, to learn from data. It’s a subset of machine learning, and it’s used to process large amounts of data and to recognize patterns or features in the data. Deep learning algorithms can identify objects, recognize speech, and even interpret natural language. \\n\\nThe history of deep learning dates back to the 1950s, when the first artificial neural networks were developed. Since then, there have been major milestones in deep learning. One of the biggest milestones was the development of convolutional neural networks in the 1980s, which enabled computers to identify objects in images. In 2012, a team of researchers developed a deep learning algorithm that was able to recognize objects more accurately than humans. This breakthrough was a major turning point in the development of deep learning. \\n\\nToday, deep'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval QA chain\n",
        "\n",
        "Retrieval QA chain is considered one of the most important helping with doing QA over your document data\n",
        "\n",
        "Here the documents can be pdf, webpages, youtube videos, notion documents etc.\n",
        "\n",
        "Let's try to understand it with the help of an example\n",
        "\n",
        "First let's download the data needed to perform QA with"
      ],
      "metadata": {
        "id": "7g5R7PSzxufT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2t4OJbiwSTo",
        "outputId": "6e917d89-8f76-4493-d5e6-f3edc31427e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-03 12:38:21--  https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39027 (38K) [text/plain]\n",
            "Saving to: ‘state_of_the_union.txt’\n",
            "\n",
            "\rstate_of_the_union.   0%[                    ]       0  --.-KB/s               \rstate_of_the_union. 100%[===================>]  38.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-03 12:38:21 (97.8 MB/s) - ‘state_of_the_union.txt’ saved [39027/39027]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the data from text document into Document format in Langchain"
      ],
      "metadata": {
        "id": "cxI02EUAK62w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "loader = TextLoader('state_of_the_union.txt', encoding='utf8')\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "X_nA6nanw9AT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create embeddings needed for this document data"
      ],
      "metadata": {
        "id": "Bl9exS9rLBw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = Chroma.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "id": "3-_wQAsELBNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's crete a RetrievalQA chain which can operate QA above the document data along with running a sample query.\n",
        "\n",
        "As you can see it is able to answer the query based on the document data it is trained on"
      ],
      "metadata": {
        "id": "7eNi2IwzLIGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "bg4BnTCZxOTr",
        "outputId": "30ba3579-2adf-4556-9720-a83adc602c4e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LoadSummarize chain\n",
        "\n",
        "This is the fundamental chain to perform the task of summarization over documents.\n",
        "\n",
        "This algorithm works by first splitting the entire input into small chunks using a text splitter. Then we create a summary of each of these chunks. Once we get a summary of each, the algorithm creates a summary over these summaries and provides an output\n",
        "\n",
        "Let's understand it with an example"
      ],
      "metadata": {
        "id": "byhCZwxSxw7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's prepare the text"
      ],
      "metadata": {
        "id": "lUJd-7nf39tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, PromptTemplate, LLMChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "text_splitter = CharacterTextSplitter()\n",
        "\n",
        "with open(\"state_of_the_union.txt\") as f:\n",
        "    state_of_the_union = f.read()\n",
        "texts = text_splitter.split_text(state_of_the_union)\n",
        "\n",
        "docs = [Document(page_content=t) for t in texts]"
      ],
      "metadata": {
        "id": "3hPtoQBrxZt4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a load_summarize chain with chain_type map_reduce . There are other chain types like stuff, refine etc."
      ],
      "metadata": {
        "id": "5t7G1PnWMKKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "chain.run(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "vqbB_oSX4CWY",
        "outputId": "37bcdcf4-9de1-448f-d037-125fae0c4e7c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" In this speech, the speaker addresses the American people and the world, discussing the recent aggression of Russia's Vladimir Putin in Ukraine and the United States' response. The speaker outlines the economic sanctions and other measures taken to hold Putin accountable, and announces the U.S. Department of Justice's task force to go after the crimes of Russian oligarchs. The speaker also discusses the American Rescue Plan, the Bipartisan Infrastructure Law, and the Bipartisan Innovation Act, and proposes a plan to fight inflation. The speaker also pays tribute to Officer Wilbert Mora and Officer Jason Rivera, who were killed in the line of duty, and nominates Circuit Court of Appeals Judge Ketanji Brown Jackson to the Supreme Court. The speech concludes with a call to fund ARPA-H, the Advanced Research Projects Agency for Health, and a message of optimism for the future of America.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Router Chain\n",
        "\n",
        "Router Chain is useful when you have multiple chains for different tasks and you wish to invoke them based on the input provided\n",
        "\n",
        "For example if you wish to do summuarization / Q&A based on the input provided you can do that with the help of"
      ],
      "metadata": {
        "id": "bg3rjTKTrSLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
        "You are so good because you are able to break down hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template,\n",
        "    },\n",
        "]\n",
        "\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
        "\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "chain = MultiPromptChain(\n",
        "    router_chain=router_chain,\n",
        "    destination_chains=destination_chains,\n",
        "    default_chain=default_chain,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(chain.run(\"What is black body radiation?\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jxd4gT5ndm5",
        "outputId": "1a9bf75b-d1b6-4c6e-96be-5ba6a2d68ca5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Black body radiation is the electromagnetic radiation emitted by a black body in thermodynamic equilibrium. It is a type of thermal radiation, and is the result of the thermal energy of the body being converted into electromagnetic radiation. The spectrum of the radiation is determined by the temperature of the body, and is a function of the body's emissivity.\n"
          ]
        }
      ]
    }
  ]
}