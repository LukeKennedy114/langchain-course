{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamurAIGPT/langchain-course/blob/main/chains/Chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9sM36nIUwvhZ"
      },
      "source": [
        "# Chains\n",
        "\n",
        "Chains are the core of LangChain. They are simply a chain of components, executed in a particular order.\n",
        "\n",
        "Langchain supports a bunch of chains each with it's own functionality. The easiest of these chains is the LLMChain . It works by taking a user's input, passing in to the first element in the chain — a PromptTemplate — to format the input into a particular prompt."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MukosxspiHgo"
      },
      "source": [
        "### LLMChain\n",
        "\n",
        "LLMChain is most basic chain in Langchain which we will talk about.\n",
        "\n",
        "It takes in a prompt template, formats it with the user input and returns the response from an LLM.\n",
        "\n",
        "#### What is the advantage of using this chain ?\n",
        "\n",
        "Using templates as input we can dynamically change what input we provide to LLM and thus use it directly without writing from scratch. Let's discuss LLMChain with an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5h56oEFjXyd"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai tiktoken chromadb python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PbdnRkUqvH6Q"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Give me a tweet idea on {topic}?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEDIHoFsjUpg",
        "outputId": "9bb47afa-3d64-4dbe-bb91-d264e8a62d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\"Technology isn't taking over the world, #AI is showing us a better version of it. Embrace the power of AI to make the future brighter!\" #AI #Future\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"AI\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "suU9b4CRkPmr"
      },
      "source": [
        "Now let's extend the same concept for multiple variables as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iez9G73Hjl0e",
        "outputId": "f8047569-1978-46c9-e294-58322784619e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\"Harness the power of #AI and #NLP to explore the fascinating world of language and unlock new insights! #AIinLanguageProcessing\"\n"
          ]
        }
      ],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic1\", \"topic2\"],\n",
        "    template=\"Give me a tweet idea on {topic1} and {topic2}?\",\n",
        ")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "print(chain.run({\n",
        "    'topic1': \"AI\",\n",
        "    'topic2': \"NLP\"\n",
        "    }))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OcFXfTTgloIW"
      },
      "source": [
        "### Sequential Chain\n",
        "\n",
        "A sequential chain works by combining two or more chains. For example you can take the output from one chain and pass it across to next chains sequentially\n",
        "\n",
        "Let's try to understand it with the help of an example"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0kLqBGnDn6b9"
      },
      "source": [
        "First we create a LLMChain to write outline for a blog topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xz2UbjHekLT5"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=.7)\n",
        "template = \"\"\"Write a blog outline given a topic.\n",
        "\n",
        "Topic: {topic}\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
        "outline_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"outline\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6roHTswvn_J7"
      },
      "source": [
        "Then we create another LLMChain which will take the output from above chain as input and creates a blog article based on the generated outline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zzzJOakWmmJO"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=.7)\n",
        "template = \"\"\"Write a blog article based on the below outline.\n",
        "\n",
        "Outline:\n",
        "{outline}\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"outline\"], template=template)\n",
        "article_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"article\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6VXf0eoMokgp"
      },
      "source": [
        "Finally let's connect both the chains using a SequentialChain which has topic as input and outline, article as output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tlFKIKdanH0D"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[outline_chain, article_chain],\n",
        "    input_variables=[\"topic\"],\n",
        "    output_variables=[\"outline\", \"article\"],\n",
        "    verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WJC0omAnVf7",
        "outputId": "a0459e36-0439-4abe-a5a2-07ddf396e66d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'topic': 'Deep Learning',\n",
              " 'outline': ' and Its Applications\\n\\nI. Introduction \\nA. Definition of Deep Learning\\nB. Overview of Deep Learning Applications\\n\\nII. What is Deep Learning? \\nA. Overview of Machine Learning\\nB. Types of Deep Learning Algorithms\\n\\nIII. Applications of Deep Learning \\nA. Computer Vision \\nB. Natural Language Processing \\nC. Robotics \\nD. Autonomous Driving \\n\\nIV. Benefits of Deep Learning \\nA. Improved Accuracy \\nB. Increased Efficiency \\nC. Faster Processing \\n\\nV. Challenges of Deep Learning \\nA. Unstructured Data \\nB. Complexity of Algorithms \\nC. Lack of Resources \\n\\nVI. Conclusion \\nA. Summary of Deep Learning \\nB. Future of Deep Learning',\n",
              " 'article': ' \\n\\n\\nDeep learning is a form of artificial intelligence that has become increasingly popular in recent years. It is a subset of machine learning, where neural networks learn from large amounts of data in order to make decisions and predictions. Deep learning has shown promise in many areas, ranging from computer vision to natural language processing and autonomous driving. \\n\\nSo, what is deep learning and how does it work? Deep learning is based on the concept of artificial neural networks, which are loosely based on the human brain. These networks are composed of layers of interconnected nodes that take in data and use mathematical models to process it and make predictions. There are several types of deep learning algorithms that can be used, such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and generative adversarial networks (GANs).\\n\\nThe applications of deep learning are vast and varied. In the field of computer vision, deep learning can be used to identify objects in images and video, as well as to generate new images. In natural language processing, it can be used to create chatbots and automatic translation services. In robotics, deep learning can be used to enable autonomous navigation and manipulation. And in autonomous driving, deep learning is used to recognize objects in the'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "overall_chain({\"topic\":\"Deep Learning\"})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7g5R7PSzxufT"
      },
      "source": [
        "### Retrieval QA chain\n",
        "\n",
        "Retrieval QA chain is considered one of the most important helping with doing QA over your document data\n",
        "\n",
        "Here the documents can be pdf, webpages, youtube videos, notion documents etc.\n",
        "\n",
        "Let's try to understand it with the help of an example\n",
        "\n",
        "First let's download the data needed to perform QA with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2t4OJbiwSTo",
        "outputId": "6e917d89-8f76-4493-d5e6-f3edc31427e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-07-04 03:38:31--  https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39027 (38K) [text/plain]\n",
            "Saving to: ‘state_of_the_union.txt’\n",
            "\n",
            "state_of_the_union. 100%[===================>]  38.11K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2023-07-04 03:38:31 (4.83 MB/s) - ‘state_of_the_union.txt’ saved [39027/39027]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cxI02EUAK62w"
      },
      "source": [
        "Let's load the data from text document into Document format in Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X_nA6nanw9AT"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "loader = TextLoader('state_of_the_union.txt', encoding='utf8')\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl9exS9rLBw8"
      },
      "source": [
        "Now let's create embeddings needed for this document data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3-_wQAsELBNh"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = Chroma.from_documents(texts, embeddings)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7eNi2IwzLIGQ"
      },
      "source": [
        "Now let's crete a RetrievalQA chain which can operate QA above the document data along with running a sample query.\n",
        "\n",
        "As you can see it is able to answer the query based on the document data it is trained on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "bg4BnTCZxOTr",
        "outputId": "30ba3579-2adf-4556-9720-a83adc602c4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" The president said Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said she is a consensus builder and has received support from the Fraternal Order of Police and former judges appointed by Democrats and Republicans.\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "qa.run(query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "byhCZwxSxw7v"
      },
      "source": [
        "### LoadSummarize chain\n",
        "\n",
        "This is the fundamental chain to perform the task of summarization over documents.\n",
        "\n",
        "This algorithm works by first splitting the entire input into small chunks using a text splitter. Then we create a summary of each of these chunks. Once we get a summary of each, the algorithm creates a summary over these summaries and provides an output\n",
        "\n",
        "Let's understand it with an example"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lUJd-7nf39tH"
      },
      "source": [
        "Let's prepare the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3hPtoQBrxZt4"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI, PromptTemplate, LLMChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "text_splitter = CharacterTextSplitter()\n",
        "\n",
        "with open(\"state_of_the_union.txt\") as f:\n",
        "    state_of_the_union = f.read()\n",
        "texts = text_splitter.split_text(state_of_the_union)\n",
        "\n",
        "docs = [Document(page_content=t) for t in texts]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5t7G1PnWMKKu"
      },
      "source": [
        "Now let's create a load_summarize chain with chain_type map_reduce . There are other chain types like stuff, refine etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "vqbB_oSX4CWY",
        "outputId": "37bcdcf4-9de1-448f-d037-125fae0c4e7c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" In this speech, the speaker addresses the American people and the world, discussing the recent aggression of Russia's Vladimir Putin in Ukraine and the United States' response. The speaker outlines the economic sanctions and other measures taken to hold Putin accountable, and announces the U.S. Department of Justice's task force to go after the crimes of Russian oligarchs. The speaker also discusses the American Rescue Plan, the Bipartisan Infrastructure Law, and the Bipartisan Innovation Act, and proposes a plan to fight inflation. The speaker also pays tribute to Officer Wilbert Mora and Officer Jason Rivera, who were killed in the line of duty, and nominates Circuit Court of Appeals Judge Ketanji Brown Jackson to the Supreme Court. The speech concludes with a call to fund ARPA-H, the Advanced Research Projects Agency for Health, and a message of optimism for the future of America.\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "chain.run(docs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bg3rjTKTrSLE"
      },
      "source": [
        "### Router Chain\n",
        "\n",
        "Router Chain is useful when you have multiple chains for different tasks and you wish to invoke them based on the input provided\n",
        "\n",
        "For example if you have two LLM chains one good at physics and one good at maths. If you ask a question, the router chain can figure out the topic and send the request to corresponding chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jxd4gT5ndm5",
        "outputId": "1a9bf75b-d1b6-4c6e-96be-5ba6a2d68ca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Black body radiation is the electromagnetic radiation emitted by a black body in thermodynamic equilibrium. It is a type of thermal radiation, and is the result of the thermal energy of the body being converted into electromagnetic radiation. The spectrum of the radiation is determined by the temperature of the body, and is a function of the body's emissivity.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
        "You are so good because you are able to break down hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template,\n",
        "    },\n",
        "]\n",
        "\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
        "\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "chain = MultiPromptChain(\n",
        "    router_chain=router_chain,\n",
        "    destination_chains=destination_chains,\n",
        "    default_chain=default_chain,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(chain.run(\"What is black body radiation?\"))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP7fn6puuqAJn4ZLHAiwHxB",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
